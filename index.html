---
layout: default
---
<!-- 
<figure>
    <center><img src="workshop_figure.gif" style="width:65%"; /></center>
    <figcaption style='text-align: center'><small>Source: <a href="https://ai.googleblog.com/2020/04/an-optimistic-perspective-on-offline.html">Google AI Blog </a></small></figcaption> -->
<!-- </figure>  --> 

<div class="row">



<p>
Reinforcement Learning has been successful in solving tasks in a wide range of fields including robotics [1], finance [2], gameplay [3], and managing control systems [4]. However, this success is limited to tasks that can be described  via carefully crafted reward functions. For the success of RL algorithms, it is crucial to design a reward function that aligns with the human intent for the task and is suitably dense. As demonstrated by several works [5,6,7,8], constructing such reward functions is not easy. The problem only becomes more pronounced when the agent is expected to solve multiple tasks. Additionally, the heavy dependence of RL agents on reward-annotated environment interactions forces the agent to explore every time the reward function changes. The perils of reward design pose the unresolved question of what and how an agent can learn from the often substantial quantity of reward-free interactions with the environment, as well as with alternative learning signals. Prior works have investigated this question by using reward-free interactions for Intrinsic Motivation [9,10], Contrastive Learning [11, 12], Skill Discovery [13, 14], and Representation Learning [15, 16]. The difficulty of reward function design has also motivated prior art to propose alternative learning signals that can make learning easier such as expert demonstrations [17, 18], preferences [19, 20], implicit human feedback [21,22,23], or target distributions [9, 24] to describe the task to an RL agent.  
</p>
<p>
This workshop investigates the following core premise: reward-free interactions are abundant in the real world, often generated both by AI systems and humans. Indeed, the difficulty of reward design can be sidestepped by considering alternative learning signals that are easy to collect such as demonstrations or preferences. With the success of language and vision  in leveraging large data to create generalist agents, one of the key objectives of this workshop is to ask the question of how far we can progress in the goal of creating similar generalist behavior agents using reward-free interactions. We hope to facilitate a confluence between researchers working on these two often disjoint areasâ€”those exploring how to make the best use of reward-free interactions and those exploring scalable alternative learning signals to make agents more capable. The discussions sparked by the ongoing work in this space can inspire novel algorithms/methods/benchmarks that would allow us to take a step in making training general-purpose RL agents more practical.  
</p>
<p>
Our invited speakers are researching reward-free RL algorithms, from self-supervised learning in RL to unsupervised pretraining methods, from preference learning to learning from observations, as well as speakers whose focus areas present new and interesting decision-making challenges where structured reward functions are not readily available. Our panel discussions and Q&A sessions focus on understanding the key foundational research challenges in how reward-free objectives can be used to leverage existing data and applied to novel applications. 
</p>






<p>
<b> Topics of Interest </b>. We will encourage discussions and contributions around (but not limited to) the following topics that are important in the context of utilizing reward-free RL:</p>
<ul>
<li> Reward-Free Task Specification: Learning from  preferences, language, cross-embodiment, social constraints, safety, demonstrations, implicit human feedback. </li>
<li> Utilizing large-scale reward-free data: Learning representations, skills, from video, and novel datasets. </li>
<li> Utilizing foundational models for efficient adaptation/finetuning.</li>
<li> Using reward-free interactions: exploration, sample efficiency, unsupervised skill learning, self-supervised objectives, goal-conditioning, and model learning for RL. </li>
</ul>
</div>

<!-- <p>
Submission site: <a href="">  </a>
The submission deadline is October 2, 2022 (Anywhere on Earth) <strike> September 30, 2022 </strike> . Please refer to the <a href="https://offline-rl-neurips.github.io/2022/submit.html"> submission </a> page for more details.	
</p> -->
	
<!-- <div id="PC" class="row">
<h3>Program Committee</h3>
<div class="break"></div>
	<ul style="width:25%; float:left; display: inline; ">
		<li>Adam R Villaflor </li>
		<li>Alex Irpan </li>


	 </ul>

	<ul style="width:25%; float:center; display: inline; ">
		<li>Ilya Kostrikov </li>

	</ul>

	<ul style="width:25%; float:right; display: inline;">
		<li>Kamyar Ghasemipour </li>
		<li>Shangtong Zhang </li>

	</ul>
</div>
 -->
<div id="organizers" class="row">
<h2 style="float:left;">Organizers</h2>
<div class="break"></div>
<div style="text-align: left;">
{%- for person in site.data.organizers -%}
<div class="person">
  <img src="{{ person.image }}" height="170px" /><div style="height:12px;"></div>
   <a href="{{ person.url | relative_url }}">{{ person.name }}</a> <div style="height:4px;"></div>
   <span>{{ person.title | replace: '&', '<div style="height:4px;"></div>' }}</span>
</div>
{%- endfor -%}
</div>
</div>
<p> To contact the organizers, please send an email to <a href="mailto:rlbrew-workshop@gmail.com">rlbrew.workshop@gmail.com</a>. <p>

